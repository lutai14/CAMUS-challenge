{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import optim, nn, utils, Tensor\n",
    "from torchvision.transforms import ToTensor\n",
    "import pytorch_lightning as pl\n",
    "import torchvision.transforms.functional as TF\n",
    "import torch.nn.functional as F\n",
    "import monai\n",
    "from unet import UNet"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import SimpleITK as sitk\n",
    "from torchvision.transforms.functional import resize, center_crop\n",
    "from torchvision.transforms import InterpolationMode\n",
    "from vicreg import VICReg, train_vicreg\n",
    "\n",
    "class CamusDataset(Dataset):\n",
    "    def __init__(self, data_path, image_size=(512, 512)):\n",
    "        super().__init__()\n",
    "        self.root = data_path\n",
    "\n",
    "        self.data_list = []\n",
    "        self.image_size = image_size\n",
    "\n",
    "        self.num_imgs = len(os.listdir(self.root))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_imgs\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        chambers = '2CH' if torch.rand(1) > 0.5 else '4CH'\n",
    "\n",
    "        path = os.path.join(self.root, f'patient{idx+1:04d}', f'patient{idx+1:04d}_{chambers}_sequence')\n",
    "\n",
    "        image_sitk = sitk.ReadImage(f'{path}.mhd', sitk.sitkFloat32)\n",
    "\n",
    "        # get pixel spacing to correct aspect ratio\n",
    "        spacing = image_sitk.GetSpacing()\n",
    "        aspect_ratio = spacing[1]/spacing[0]\n",
    "\n",
    "        # convert to numpy\n",
    "\n",
    "        first_frame_index = torch.randint(0, image_sitk.GetSize()[2] - 2, (1,))\n",
    "\n",
    "        image_1 = sitk.GetArrayFromImage(image_sitk)[first_frame_index] / 255\n",
    "        image_2 = sitk.GetArrayFromImage(image_sitk)[first_frame_index + 1] / 255\n",
    "\n",
    "\n",
    "\n",
    "        # compute aspect ratio of pixel(mm) and image(pixels)\n",
    "        pixel_aspect = spacing[1] / spacing[0]\n",
    "        image_aspect = image_sitk.GetHeight() / image_sitk.GetWidth()\n",
    "\n",
    "        # preprocess image and mask\n",
    "\n",
    "        image_1, image_2 = torch.Tensor(image_1).unsqueeze(0), torch.Tensor(image_2).unsqueeze(0)\n",
    "        size =  (self.image_size[0], int(image_1.shape[2]*image_aspect*pixel_aspect))\n",
    "\n",
    "        image_1  = resize(image_1, size, interpolation=InterpolationMode.BICUBIC)\n",
    "        image_2 = resize(image_2, size, interpolation=InterpolationMode.NEAREST)\n",
    "\n",
    "        image_1, image_2 = center_crop(image_1, self.image_size), center_crop(image_2, self.image_size)\n",
    "\n",
    "        image = torch.cat((image_1, image_2), dim=0)\n",
    "\n",
    "        return image"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "dataset = CamusDataset('../data/training')\n",
    "model = UNet(n_channels=1, n_classes=4, bilinear=False, scaling=2)\n",
    "vicreg = VICReg(model)\n",
    "loader = utils.data.DataLoader(dataset, batch_size=3, shuffle=True)\n",
    "optimizer = optim.Adam(vicreg.parameters(), lr=5e-5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0\n",
      "Batch 0 loss: 198.98565673828125\n",
      "Batch 5 loss: 152.1251220703125\n",
      "Batch 10 loss: 134.63430786132812\n",
      "Batch 15 loss: 103.71627807617188\n",
      "Batch 20 loss: 96.26701354980469\n",
      "Batch 25 loss: 69.46277618408203\n",
      "Batch 30 loss: 78.04834747314453\n",
      "Batch 35 loss: 48.51445007324219\n",
      "Batch 40 loss: 53.67090606689453\n",
      "Batch 45 loss: 91.96554565429688\n",
      "Batch 50 loss: 37.54883575439453\n",
      "Batch 55 loss: 35.15755081176758\n",
      "Batch 60 loss: 28.968894958496094\n",
      "Batch 65 loss: 42.093666076660156\n",
      "Batch 70 loss: 40.1080322265625\n",
      "Batch 75 loss: 31.390186309814453\n",
      "Batch 80 loss: 26.47576332092285\n",
      "Batch 85 loss: 28.208036422729492\n",
      "Batch 90 loss: 26.250574111938477\n",
      "Batch 95 loss: 29.65609359741211\n",
      "Batch 100 loss: 27.12096405029297\n",
      "Batch 105 loss: 25.896150588989258\n",
      "Batch 110 loss: 25.378459930419922\n",
      "Batch 115 loss: 25.56060791015625\n",
      "Batch 120 loss: 35.1664924621582\n",
      "Batch 125 loss: 27.330699920654297\n",
      "Batch 130 loss: 27.975772857666016\n",
      "Batch 135 loss: 24.657821655273438\n",
      "Batch 140 loss: 23.884981155395508\n",
      "Batch 145 loss: 25.311803817749023\n",
      "Starting epoch 1\n",
      "Batch 0 loss: 24.859699249267578\n",
      "Batch 5 loss: 25.09037208557129\n",
      "Batch 10 loss: 24.837373733520508\n",
      "Batch 15 loss: 26.908084869384766\n",
      "Batch 20 loss: 24.545778274536133\n",
      "Batch 25 loss: 23.94342613220215\n",
      "Batch 30 loss: 24.27617645263672\n",
      "Batch 35 loss: 25.25066375732422\n",
      "Batch 40 loss: 23.999067306518555\n",
      "Batch 45 loss: 23.810850143432617\n",
      "Batch 50 loss: 24.99317741394043\n",
      "Batch 55 loss: 27.45572280883789\n",
      "Batch 60 loss: 33.294700622558594\n",
      "Batch 65 loss: 23.864120483398438\n",
      "Batch 70 loss: 23.70238494873047\n",
      "Batch 75 loss: 24.296361923217773\n",
      "Batch 80 loss: 23.683605194091797\n",
      "Batch 85 loss: 28.702098846435547\n",
      "Batch 90 loss: 24.13638687133789\n",
      "Batch 95 loss: 24.332277297973633\n",
      "Batch 100 loss: 24.365915298461914\n",
      "Batch 105 loss: 23.646289825439453\n",
      "Batch 110 loss: 23.51683235168457\n",
      "Batch 115 loss: 23.55489730834961\n",
      "Batch 120 loss: 23.783504486083984\n",
      "Batch 125 loss: 23.67704200744629\n",
      "Batch 130 loss: 23.87197494506836\n",
      "Batch 135 loss: 27.40427017211914\n",
      "Batch 140 loss: 23.55458641052246\n",
      "Batch 145 loss: 23.90423011779785\n",
      "Starting epoch 2\n",
      "Batch 0 loss: 23.539310455322266\n",
      "Batch 5 loss: 24.494396209716797\n",
      "Batch 10 loss: 23.663925170898438\n",
      "Batch 15 loss: 37.10529327392578\n",
      "Batch 20 loss: 23.830663681030273\n",
      "Batch 25 loss: 24.40007972717285\n",
      "Batch 30 loss: 24.85553741455078\n",
      "Batch 35 loss: 23.53179359436035\n",
      "Batch 40 loss: 23.57332420349121\n",
      "Batch 45 loss: 23.665184020996094\n",
      "Batch 50 loss: 23.548208236694336\n",
      "Batch 55 loss: 23.614370346069336\n",
      "Batch 60 loss: 23.572086334228516\n",
      "Batch 65 loss: 23.67681121826172\n",
      "Batch 70 loss: 23.51919174194336\n",
      "Batch 75 loss: 24.645566940307617\n",
      "Batch 80 loss: 28.255882263183594\n",
      "Batch 85 loss: 23.55294418334961\n",
      "Batch 90 loss: 24.126026153564453\n",
      "Batch 95 loss: 23.544086456298828\n",
      "Batch 100 loss: 23.553844451904297\n",
      "Batch 105 loss: 23.520143508911133\n",
      "Batch 110 loss: 23.52815055847168\n",
      "Batch 115 loss: 23.551918029785156\n",
      "Batch 120 loss: 23.514795303344727\n",
      "Batch 125 loss: 25.611099243164062\n",
      "Batch 130 loss: 23.701885223388672\n",
      "Batch 135 loss: 23.51345443725586\n",
      "Batch 140 loss: 24.891830444335938\n",
      "Batch 145 loss: 23.55471420288086\n",
      "Starting epoch 3\n",
      "Batch 0 loss: 23.516145706176758\n",
      "Batch 5 loss: 23.680814743041992\n",
      "Batch 10 loss: 23.534467697143555\n",
      "Batch 15 loss: 23.56406021118164\n",
      "Batch 20 loss: 23.5166072845459\n",
      "Batch 25 loss: 23.540660858154297\n",
      "Batch 30 loss: 23.556682586669922\n",
      "Batch 35 loss: 24.869457244873047\n",
      "Batch 40 loss: 23.686397552490234\n",
      "Batch 45 loss: 23.55739402770996\n",
      "Batch 50 loss: 23.508899688720703\n",
      "Batch 55 loss: 23.75190544128418\n",
      "Batch 60 loss: 23.50861930847168\n",
      "Batch 65 loss: 23.517358779907227\n",
      "Batch 70 loss: 23.845067977905273\n",
      "Batch 75 loss: 23.582868576049805\n",
      "Batch 80 loss: 23.534900665283203\n",
      "Batch 85 loss: 23.539106369018555\n",
      "Batch 90 loss: 23.58443832397461\n",
      "Batch 95 loss: 23.495182037353516\n",
      "Batch 100 loss: 23.48984146118164\n",
      "Batch 105 loss: 23.566204071044922\n",
      "Batch 110 loss: 23.583330154418945\n",
      "Batch 115 loss: 23.509849548339844\n",
      "Batch 120 loss: 23.573524475097656\n",
      "Batch 125 loss: 25.882389068603516\n",
      "Batch 130 loss: 23.78030014038086\n",
      "Batch 135 loss: 23.547256469726562\n",
      "Batch 140 loss: 23.5199031829834\n",
      "Batch 145 loss: 23.49717140197754\n",
      "Starting epoch 4\n",
      "Batch 0 loss: 23.47994613647461\n",
      "Batch 5 loss: 23.503673553466797\n",
      "Batch 10 loss: 23.675514221191406\n",
      "Batch 15 loss: 23.511262893676758\n",
      "Batch 20 loss: 23.481189727783203\n",
      "Batch 25 loss: 23.492704391479492\n",
      "Batch 30 loss: 23.465055465698242\n",
      "Batch 35 loss: 23.545469284057617\n",
      "Batch 40 loss: 23.54100227355957\n",
      "Batch 45 loss: 23.435449600219727\n",
      "Batch 50 loss: 23.526103973388672\n",
      "Batch 55 loss: 23.535228729248047\n",
      "Batch 60 loss: 23.62434196472168\n",
      "Batch 65 loss: 27.224105834960938\n",
      "Batch 70 loss: 23.50495147705078\n",
      "Batch 75 loss: 23.516504287719727\n",
      "Batch 80 loss: 23.53790283203125\n",
      "Batch 85 loss: 23.53487777709961\n",
      "Batch 90 loss: 23.584468841552734\n",
      "Batch 95 loss: 23.526247024536133\n",
      "Batch 100 loss: 23.51119613647461\n",
      "Batch 105 loss: 23.60523796081543\n",
      "Batch 110 loss: 23.6345272064209\n",
      "Batch 115 loss: 23.550588607788086\n",
      "Batch 120 loss: 23.464616775512695\n",
      "Batch 125 loss: 23.490781784057617\n",
      "Batch 130 loss: 23.719539642333984\n",
      "Batch 135 loss: 23.49009895324707\n",
      "Batch 140 loss: 23.47356605529785\n",
      "Batch 145 loss: 23.485095977783203\n",
      "Starting epoch 5\n",
      "Batch 0 loss: 23.532222747802734\n",
      "Batch 5 loss: 23.466651916503906\n",
      "Batch 10 loss: 23.567306518554688\n",
      "Batch 15 loss: 23.705272674560547\n",
      "Batch 20 loss: 23.512304306030273\n",
      "Batch 25 loss: 24.16888427734375\n",
      "Batch 30 loss: 23.52495765686035\n",
      "Batch 35 loss: 23.519405364990234\n",
      "Batch 40 loss: 23.49474334716797\n",
      "Batch 45 loss: 23.486082077026367\n",
      "Batch 50 loss: 23.506399154663086\n",
      "Batch 55 loss: 23.58385467529297\n",
      "Batch 60 loss: 23.572689056396484\n",
      "Batch 65 loss: 23.481945037841797\n",
      "Batch 70 loss: 23.45393180847168\n",
      "Batch 75 loss: 30.44928741455078\n",
      "Batch 80 loss: 23.534526824951172\n",
      "Batch 85 loss: 23.542203903198242\n",
      "Batch 90 loss: 23.577922821044922\n",
      "Batch 95 loss: 23.725534439086914\n",
      "Batch 100 loss: 23.876705169677734\n",
      "Batch 105 loss: 23.57620620727539\n",
      "Batch 110 loss: 23.500110626220703\n",
      "Batch 115 loss: 23.52327537536621\n",
      "Batch 120 loss: 23.489225387573242\n",
      "Batch 125 loss: 23.459152221679688\n",
      "Batch 130 loss: 23.471385955810547\n",
      "Batch 135 loss: 23.48271942138672\n",
      "Batch 140 loss: 23.50972557067871\n",
      "Batch 145 loss: 23.476354598999023\n",
      "Starting epoch 6\n",
      "Batch 0 loss: 23.556989669799805\n",
      "Batch 5 loss: 23.47951316833496\n",
      "Batch 10 loss: 23.474891662597656\n",
      "Batch 15 loss: 23.559350967407227\n",
      "Batch 20 loss: 23.519149780273438\n",
      "Batch 25 loss: 23.430980682373047\n",
      "Batch 30 loss: 55.415504455566406\n",
      "Batch 35 loss: 23.88665008544922\n",
      "Batch 40 loss: 23.61809730529785\n",
      "Batch 45 loss: 23.585552215576172\n",
      "Batch 50 loss: 23.842792510986328\n",
      "Batch 55 loss: 23.57184600830078\n",
      "Batch 60 loss: 23.6253719329834\n",
      "Batch 65 loss: 23.51869773864746\n",
      "Batch 70 loss: 23.529136657714844\n",
      "Batch 75 loss: 23.54315185546875\n",
      "Batch 80 loss: 45.17359924316406\n",
      "Batch 85 loss: 23.561309814453125\n",
      "Batch 90 loss: 23.791799545288086\n",
      "Batch 95 loss: 23.70246696472168\n",
      "Batch 100 loss: 23.59314727783203\n",
      "Batch 105 loss: 23.5639705657959\n",
      "Batch 110 loss: 23.603534698486328\n",
      "Batch 115 loss: 23.524017333984375\n",
      "Batch 120 loss: 23.55721092224121\n",
      "Batch 125 loss: 29.167367935180664\n",
      "Batch 130 loss: 23.501819610595703\n",
      "Batch 135 loss: 23.592071533203125\n",
      "Batch 140 loss: 23.644790649414062\n",
      "Batch 145 loss: 23.76261329650879\n",
      "Starting epoch 7\n",
      "Batch 0 loss: 23.50490379333496\n",
      "Batch 5 loss: 23.810678482055664\n",
      "Batch 10 loss: 23.53084945678711\n",
      "Batch 15 loss: 23.495418548583984\n",
      "Batch 20 loss: 23.500593185424805\n",
      "Batch 25 loss: 23.489656448364258\n",
      "Batch 30 loss: 23.516407012939453\n",
      "Batch 35 loss: 23.506301879882812\n",
      "Batch 40 loss: 23.645322799682617\n",
      "Batch 45 loss: 23.521879196166992\n",
      "Batch 50 loss: 23.591171264648438\n",
      "Batch 55 loss: 23.468965530395508\n",
      "Batch 60 loss: 23.511682510375977\n",
      "Batch 65 loss: 23.472488403320312\n",
      "Batch 70 loss: 23.506908416748047\n",
      "Batch 75 loss: 23.44275665283203\n",
      "Batch 80 loss: 23.439483642578125\n",
      "Batch 85 loss: 23.514585494995117\n",
      "Batch 90 loss: 23.97039794921875\n",
      "Batch 95 loss: 23.453495025634766\n",
      "Batch 100 loss: 23.50762939453125\n",
      "Batch 105 loss: 23.466035842895508\n",
      "Batch 110 loss: 23.465961456298828\n",
      "Batch 115 loss: 23.44266128540039\n",
      "Batch 120 loss: 23.418115615844727\n",
      "Batch 125 loss: 24.02506446838379\n",
      "Batch 130 loss: 23.487197875976562\n",
      "Batch 135 loss: 23.47723960876465\n",
      "Batch 140 loss: 23.427566528320312\n",
      "Batch 145 loss: 23.487314224243164\n",
      "Starting epoch 8\n",
      "Batch 0 loss: 23.447908401489258\n",
      "Batch 5 loss: 23.64813804626465\n",
      "Batch 10 loss: 23.492870330810547\n",
      "Batch 15 loss: 23.56489372253418\n",
      "Batch 20 loss: 25.1646728515625\n",
      "Batch 25 loss: 23.45755958557129\n",
      "Batch 30 loss: 23.438491821289062\n",
      "Batch 35 loss: 23.480310440063477\n",
      "Batch 40 loss: 24.13741683959961\n",
      "Batch 45 loss: 23.435272216796875\n",
      "Batch 50 loss: 23.60778045654297\n",
      "Batch 55 loss: 23.479162216186523\n",
      "Batch 60 loss: 23.423255920410156\n",
      "Batch 65 loss: 23.430830001831055\n",
      "Batch 70 loss: 23.499202728271484\n",
      "Batch 75 loss: 23.38861083984375\n",
      "Batch 80 loss: 23.4872989654541\n",
      "Batch 85 loss: 25.40371322631836\n",
      "Batch 90 loss: 23.480056762695312\n",
      "Batch 95 loss: 23.453006744384766\n",
      "Batch 100 loss: 23.44232177734375\n",
      "Batch 105 loss: 23.418367385864258\n",
      "Batch 110 loss: 23.44928550720215\n",
      "Batch 115 loss: 23.416486740112305\n",
      "Batch 120 loss: 23.373868942260742\n",
      "Batch 125 loss: 23.50612449645996\n",
      "Batch 130 loss: 23.424236297607422\n",
      "Batch 135 loss: 23.48971176147461\n",
      "Batch 140 loss: 23.43770408630371\n",
      "Batch 145 loss: 23.38220977783203\n",
      "Starting epoch 9\n",
      "Batch 0 loss: 23.453279495239258\n",
      "Batch 5 loss: 23.481054306030273\n",
      "Batch 10 loss: 23.53787612915039\n",
      "Batch 15 loss: 23.45356559753418\n",
      "Batch 20 loss: 23.402063369750977\n",
      "Batch 25 loss: 23.403654098510742\n",
      "Batch 30 loss: 31.038236618041992\n",
      "Batch 35 loss: 23.70093536376953\n",
      "Batch 40 loss: 23.624948501586914\n",
      "Batch 45 loss: 23.48220443725586\n",
      "Batch 50 loss: 23.43103790283203\n",
      "Batch 55 loss: 23.928272247314453\n",
      "Batch 60 loss: 23.655532836914062\n",
      "Batch 65 loss: 23.72332000732422\n",
      "Batch 70 loss: 23.584409713745117\n",
      "Batch 75 loss: 23.525625228881836\n",
      "Batch 80 loss: 23.494274139404297\n",
      "Batch 85 loss: 23.54372215270996\n",
      "Batch 90 loss: 23.522891998291016\n",
      "Batch 95 loss: 23.479278564453125\n",
      "Batch 100 loss: 23.502212524414062\n",
      "Batch 105 loss: 23.482437133789062\n",
      "Batch 110 loss: 24.38979148864746\n",
      "Batch 115 loss: 23.51258087158203\n",
      "Batch 120 loss: 23.64027976989746\n",
      "Batch 125 loss: 24.159095764160156\n",
      "Batch 130 loss: 23.602582931518555\n",
      "Batch 135 loss: 23.613494873046875\n",
      "Batch 140 loss: 23.50765037536621\n",
      "Batch 145 loss: 23.46106719970703\n"
     ]
    }
   ],
   "source": [
    "vicreg, losses = train_vicreg(vicreg, loader, optimizer, 10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "torch.save(vicreg.encoder.state_dict(), 'models/vicreg_encoder_big.pth')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}